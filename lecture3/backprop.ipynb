{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/UOS-COMP6252/public/blob/main/lecture3/backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation practical exercise\n",
    "Learning outcomes: you will be able to\n",
    "1. Describe how the backward graph is implemented in PyTorch using the ```grad_fn.next_functions```\n",
    "1. Explain how PyTorch uses computational graphs to implement backpropagation   \n",
    "1. Apply PyTorch ```grad_fn``` to compute the gradient of any expression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation\n",
    " - We saw that PyTorch can compute the gradient to any function. \n",
    " - To accomplish this, PyTorch uses three concepts:\n",
    " \n",
    " 1. Computational graph\n",
    " 1. \"Database\" of derivatives to **primitive** functions\n",
    " 1. Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Consider the following computations:\n",
    "$$\n",
    "\\begin{align*}\n",
    "a&=2\\\\\n",
    "b&=4\\\\\n",
    "c&=a+b\\\\\n",
    "d&=\\log(a)*\\log(b)\\\\\n",
    "e&=c*d\n",
    "\\end{align*}\n",
    "$$\n",
    " \n",
    "\n",
    " Where $\\log(x)$ is the log base two. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- For every **primitive** operation, PyTorch \"knows\" its derivative. \n",
    "- For example, PyTorch has the following rules stored and can look them up when needed.\n",
    "\n",
    "1. $\\frac{\\partial\\log(x)}{\\partial x}=\\frac{1}{x*\\ln(2)}$\n",
    "1. $\\frac{\\partial (x*y)}{\\partial x}=y$\n",
    "1. $\\frac{\\partial (x*y)}{\\partial y}=x$\n",
    "1. $\\frac{\\partial (x+y)}{\\partial x}=1$\n",
    "1. $\\frac{\\partial (x+y)}{\\partial y}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The PyTorch code corresponding to the example is shown below. Note:\n",
    "1. For each **leaf** tensor ```X```, the gradient is stored in ```X.grad```\n",
    "1. The values for **non-leaf** node ```Y``` is **not** saved, unless ```Y.retain_grad()``` is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6562) tensor(4.1640)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.tensor(2.,requires_grad=True)\n",
    "b=torch.tensor(4.,requires_grad=True)\n",
    "c=a+b\n",
    "d=torch.log2(a)*torch.log2(b)\n",
    "#d.retain_grad()\n",
    "e=c*d\n",
    "e.backward(retain_graph=True)\n",
    "print(a.grad,b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- How does ```backward``` work?\n",
    "- Recursively calls the ```.grad_fn``` of each node in the graph\n",
    "- We use ```.no_grad()``` so that PyTorch does not build a CG  for the computations below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- However, the code below will give an **error** because the graph is **destroyed** after the call to ```.backward()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of e wrt to c=2.0, wrt to d=6.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    dedc,dedd=e.grad_fn(torch.tensor(1.))\n",
    "    print(\"gradient of e wrt to c={}, wrt to d={}\".format(dedc,dedd))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What can be done?\n",
    "     - Either we rebuild the graph\n",
    "     - Or we use the ```.retain_graph=True``` in the call to ```.backward()```\n",
    "- Once that is done we see that the results are exactly as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c's contribution to grad of e wrt to a and b: (tensor(2.), tensor(2.))\n",
      "What does this output (tensor(12.), tensor(6.)) correspond to?\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"c\\'s contribution to grad of e wrt to a and b: {}\".format(c.grad_fn(dedc)))\n",
    "    print(\"What does this output {} correspond to?\".format(d.grad_fn(dedd)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Recall that the results for ```c.grad_fn``` are added to ```a``` and ```b```\n",
    "- but the results for ```d.grad_fn``` are fed to **unnamed** intermediate nodes\n",
    "- How does one proceed? How does PyTorch keep track?\n",
    "- Actually PyTorch keeps track of the next ```grad_fn``` to call using ```next_functions``` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<Log2Backward0 object at 0x7f7fc00eae30>, 0), (<Log2Backward0 object at 0x7f7fc00ead70>, 0))\n"
     ]
    }
   ],
   "source": [
    "print(d.grad_fn.next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- it is a tuple of pairs of the form (function,index)\n",
    "- We can almost always ignore the index. For the vast majority of operations there is only one value 0\n",
    "- Also, since most operations are binary the tuple is usually a pair\n",
    "- In summary, for most operations that we encounter ```.next_functions``` returns a pair of pairs where the idex in each pair can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of e wrt to c is=2.0, wrt to d=6.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #the gradients are accumulated, we need to reset\n",
    "    a.grad,b.grad=None,None\n",
    "    # The \"root\" of the graph always has gradient of 1\n",
    "    dedc,dedd=e.grad_fn(torch.tensor(1.))\n",
    "    print(\"gradient of e wrt to c is={}, wrt to d={}\".format(dedc,dedd))\n",
    "    #gradient functions of C and D\n",
    "    CgradFn=e.grad_fn.next_functions[0][0]\n",
    "    DgradFn=e.grad_fn.next_functions[1][0]\n",
    "    # apply to the last values\n",
    "    deda,dedb=CgradFn(dedc)\n",
    "    dedloga,dedlogb=DgradFn(dedd)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contribution of c branch to grad of a=2.0, to grad of b=2.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "      \n",
    "    # the \"C\" branch ends and accumulates in .grad\n",
    "    accA1=CgradFn.next_functions[0][0]\n",
    "    accB1=CgradFn.next_functions[1][0]\n",
    "    accA1(deda)\n",
    "    accB1(dedb)\n",
    "    print(\"contribution of c branch to grad of a={}, to grad of b={}\".format(a.grad,b.grad))\n",
    "    # the \"D\" branch has unamed nodes, loga and logb\n",
    "    LogAgradFn=DgradFn.next_functions[0][0]\n",
    "    LogBgradFn=DgradFn.next_functions[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6562) tensor(4.1640)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #apply to the last values\n",
    "    deda=LogAgradFn(dedloga)\n",
    "    dedb=LogBgradFn(dedlogb)\n",
    "    # the \"D\" branch ends and accumulates in .grad\n",
    "    accA=LogAgradFn.next_functions[0][0]\n",
    "    accA(deda)\n",
    "    accB=LogBgradFn.next_functions[0][0]\n",
    "    accB(dedb)\n",
    "    print(a.grad,b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Rather than perform all the above operations manually, we can write a recursive function to go over all the nodes in the graph and computes the grads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def preorder(grad_fn,input,debug=False):\n",
    "    vals=grad_fn(input)\n",
    "    if debug:\n",
    "        print(grad_fn,input)\n",
    "    if isinstance(vals,tuple):\n",
    "        for (f,idx),val in zip(grad_fn.next_functions,vals):\n",
    "            preorder(f,val,debug)\n",
    "    else:\n",
    "        # in this example the \"log(a)\" and \"log(b)\" return a single output\n",
    "        # see slides page 14\n",
    "        if debug:\n",
    "            print(f\"single return {grad_fn.next_functions[0][0]}\")\n",
    "        preorder(grad_fn.next_functions[0][0],vals,debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    a.grad,b.grad=None,None\n",
    "    preorder(e.grad_fn,torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6562) tensor(4.1640)\n"
     ]
    }
   ],
   "source": [
    "print(a.grad,b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x7f7fc00ebf40> tensor(1.)\n",
      "<AddBackward0 object at 0x7f7ef36eee00> tensor(2.)\n",
      "<AccumulateGrad object at 0x7f7fc00eab60> tensor(2.)\n",
      "<AccumulateGrad object at 0x7f7fc00ebe50> tensor(2.)\n",
      "<MulBackward0 object at 0x7f7ef36ef0d0> tensor(6.)\n",
      "<Log2Backward0 object at 0x7f7fc4201420> tensor(12.)\n",
      "single return <AccumulateGrad object at 0x7f7fc00eab60>\n",
      "<AccumulateGrad object at 0x7f7fc00eab60> tensor(8.6562)\n",
      "<Log2Backward0 object at 0x7f7fc4200880> tensor(6.)\n",
      "single return <AccumulateGrad object at 0x7f7fc00ebe50>\n",
      "<AccumulateGrad object at 0x7f7fc00ebe50> tensor(2.1640)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    a.grad,b.grad=None,None\n",
    "    preorder(e.grad_fn,torch.tensor(1.),debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In fact this is exactly how the ```torchviz``` package works (with more bells and whistles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import torchviz\n",
    "except ModuleNotFoundError:\n",
    "    %pip install torchviz\n",
    "    import torchviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "a=torch.tensor(2.,requires_grad=True)\n",
    "b=torch.tensor(4.,requires_grad=True)\n",
    "c=a+b\n",
    "d=torch.log2(a)*torch.log2(b)\n",
    "e=c*d\n",
    "torchviz.make_dot(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
